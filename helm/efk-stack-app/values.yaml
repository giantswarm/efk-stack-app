global:
  registry: quay.io

  opendistro:
    es:
      client:
        uri: efk-stack-app-opendistro-es-client-service

opendistro-certs:
  enabled: true
  organization: Giant Swarm
  expiration: 5 years

elasticsearch-curator:
  enabled: true
  env:
    ELASTIC_CREDS: $(ELASTIC_USER):$(ELASTIC_PASS)
  envFromSecrets:
    ELASTIC_USER:
      from:
        secret: kibana-auth
        key: username
    ELASTIC_PASS:
      from:
        secret: kibana-auth
        key: password
  command: ["/usr/bin/curator"]
  image:
    # https://quay.io/repository/giantswarm/bobrik-curator?tab=tags
    # https://github.com/elastic/curator
    registry: quay.io
    repository: giantswarm/bobrik-curator
    tag: 5.8.1

  configMaps:
    # Delete indices older than 7 days
    action_file_yml: |-
      ---
      actions:
        1:
          action: delete_indices
          description: "Clean up ES by deleting old indices"
          options:
            timeout_override:
            continue_if_exception: False
            disable_action: False
            ignore_empty_list: True
          filters:
          - filtertype: age
            source: name
            direction: older
            timestring: '%Y.%m.%d'
            unit: days
            unit_count: 7
            field:
            stats_result:
            epoch:
            exclude: False
    # Having config_yaml WILL override the other config
    # Check workaround https://github.com/elastic/curator/issues/1440#issuecomment-522409936
    config_yml: |-
      ---
      client:
        hosts:
          - efk-stack-app-opendistro-es-client-service
        port: 9200
        http_auth: ${ELASTIC_CREDS}

elasticsearch-exporter:
  enabled: true
  image:
    # see https://quay.io/repository/giantswarm/elasticsearch_exporter?tab=tags
    # and https://github.com/justwatchcom/elasticsearch_exporter
    registry: quay.io
    repository: giantswarm/elasticsearch_exporter
    tag: 1.1.0
  service:
    annotations:
      giantswarm.io/monitoring-path: /metrics
      giantswarm.io/monitoring-port: "9108"
    labels:
      giantswarm.io/monitoring: "true"
  envFromSecret: "kibana-auth"
  es:
    uri: http://$(username):$(password)@efk-stack-app-opendistro-es-client-service:9200

fluentd-elasticsearch:
  enabled: true
  configMaps:
    useDefaults:
      systemConf: true
      systemInputConf: false
  podSecurityPolicy:
    enabled: true
  image:
    # see https://quay.io/repository/giantswarm/fluentd?tab=tags
    # and https://github.com/kubernetes/kubernetes/tree/master/cluster/addons/fluentd-elasticsearch/fluentd-es-image
    registry: quay.io
    repository: giantswarm/fluentd
    tag: v3.3.0-for-opendistro
  elasticsearch:
    host: "efk-stack-app-opendistro-es-client-service"
    logstashPrefix: "fluentd"
    sslVerify: false
    scheme: "http"
    auth:
      enabled: true
  env:
    LD_PRELOAD: /usr/lib/x86_64-linux-gnu/libjemalloc.so.2
  secret:
    - name: OUTPUT_USER
      secret_name: kibana-auth
      secret_key: username
    - name: OUTPUT_PASSWORD
      secret_name: kibana-auth
      secret_key: password

opendistro-es:
  enabled: true
  # fullnameOverride: opendistro-es
  # -> opendistro-es-client

  kibana:
    enabled: true
    # Allows override of the registry for the kibana image only
    registry: ""
    # see https://quay.io/repository/giantswarm/opendistro-for-elasticsearch-kibana?tab=tags
    image: giantswarm/opendistro-for-elasticsearch-kibana
    imageTag: 1.13.2
    imagePullPolicy: IfNotPresent

    config:
      server:
        name: kibana
        host: "0"

      elasticsearch:
        hosts: http://efk-stack-app-opendistro-es-client-service:9200
        requestTimeout: 360000
        username: ${ELASTICSEARCH_USERNAME}
        password: ${ELASTICSEARCH_PASSWORD}

      elasticsearch.requestHeadersWhitelist: ["securitytenant", "Authorization"]

      logging.verbose: false

      opendistro_security:
        cookie.secure: true
        cookie.password: ${COOKIE_PASS}
        # Multitenancy with global/private tenants disabled,
        # set to both to true if you want them to be available.
        multitenancy.enabled: false
        multitenancy.tenants.enable_private: false
        multitenancy.tenants.enable_global: false
        readonly_mode.roles: ["kibana_read_only"]

    elasticsearchAccount:
      secret: kibana-auth

  elasticsearch:
    # see https://quay.io/repository/giantswarm/opendistro-for-elasticsearch?tab=tags
    image: giantswarm/opendistro-for-elasticsearch
    imageTag: 1.13.3
    imagePullPolicy: IfNotPresent

    sysctl:
      enabled: true

    initContainer:
      # see https://quay.io/repository/giantswarm/busybox?tab=tags
      image: giantswarm/busybox
      imageTag: 1.33.0

    ssl:
      transport:
        enabled: true
        existingCertSecret: opendistro-transport-certs

    master:
      replicas: 3

      persistence:
        size: 50Gi

      podDisruptionBudget:
        enabled: true
        # minAvailable: null
        maxUnavailable: 1

      affinity:
        podAntiAffinity:
          preferredDuringSchedulingIgnoredDuringExecution:
          - weight: 1
            podAffinityTerm:
              topologyKey: "kubernetes.io/hostname"
              labelSelector:
                matchLabels:
                  role: master

    data:
      replicas: 3

      persistence:
        size: 100Gi

      podDisruptionBudget:
        enabled: true
        # minAvailable: null
        maxUnavailable: 1

      affinity:
        podAntiAffinity:
          preferredDuringSchedulingIgnoredDuringExecution:
          - weight: 1
            podAffinityTerm:
              topologyKey: "kubernetes.io/hostname"
              labelSelector:
                matchLabels:
                  role: data

    client:
      replicas: 2

      ingress:
        enabled: false

      podDisruptionBudget:
        enabled: true
        # minAvailable: null
        maxUnavailable: 1

      affinity:
        podAntiAffinity:
          preferredDuringSchedulingIgnoredDuringExecution:
          - weight: 1
            podAffinityTerm:
              topologyKey: "kubernetes.io/hostname"
              labelSelector:
                matchLabels:
                  role: client

    config:
      # Majority of options described here: https://github.com/opendistro-for-elasticsearch/security/blob/master/securityconfig/elasticsearch.yml.example
      opendistro_security:
        audit:
          ignore_users: ["admin"]
          # See: https://opendistro.github.io/for-elasticsearch-docs/docs/security-audit-logs/
          type: internal_elasticsearch

        allow_unsafe_democertificates: false
        # Set to false if running securityadmin.sh manually following deployment
        allow_default_init_securityindex: true
        # See: https://github.com/opendistro-for-elasticsearch/security/blob/master/securityconfig/elasticsearch.yml.example#L27
        roles_mapping_resolution: BOTH
        restapi.roles_enabled: ["all_access", "security_rest_api_access"]
        # See: https://github.com/opendistro-for-elasticsearch/security/blob/master/securityconfig/elasticsearch.yml.example#L17
        nodes_dn:
          - "*"
        ssl:
          transport:
            enforce_hostname_verification: false
            pemcert_filepath: elk-transport-crt.pem
            pemkey_filepath: elk-transport-key.pem
            pemtrustedcas_filepath: elk-transport-root-ca.pem
